{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6274b702-c968-4950-b0b2-84643aef4a34",
   "metadata": {},
   "source": [
    "# 小作业三： 夏普利值、梯度上升法与简单多智能体学习算法\n",
    "\n",
    "同学们此前的课上已经了解了夏普利值与一些经典的多智能体学习算法。在这次作业用，你们将会回顾之前的知识，并用python来加深对这些知识的理解。\n",
    "\n",
    "## 目录\n",
    "\n",
    "本次作业主要分为以下几个部分：\n",
    "- 夏普利值\n",
    "    - 夏普利值计算\n",
    "- 矩阵博弈\n",
    "    - 矩阵博弈环境初始化\n",
    "    - 梯度上升法之IGA\n",
    "    - 梯度上升法之WoLF-IGA\n",
    "    - 梯度上升法之IGA-PP\n",
    "- 随机博弈\n",
    "    - 随机博弈环境初始化\n",
    "    - Q-Learning\n",
    "    - 多智能体学习算法扩展\n",
    "\n",
    "## 提交说明：\n",
    "请同学们在canvas上提交，本次作业满分100分，占总成绩15%。\n",
    "\n",
    "**注意，请同学们在提交的版本中不要添加和删改notebook中的函数名，我们会根据这些实现的函数来进行评分。如有其他问题，请联系助教。**\n",
    "\n",
    "提交文件名设置为 `{姓名}_{学号}_hw3.ipynb`，如`小明_123_hw3.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78273e18",
   "metadata": {},
   "source": [
    "## 夏普利值（共20分）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881b4f6",
   "metadata": {},
   "source": [
    "今有A、B、C三位商人考虑合作完成一系列订单。对于每个订单，A、B、C单干时分别能获利0、0、1万元，A、B合作共能获利6万元，A、C合作共能获利5万元，B、C合作则共能获利3万元。今A、B、C三人共同合作完成了一个订单，一共获利了10万元，按照夏普利的方法，三人该如何分配这10万获利？\n",
    "\n",
    "为方便表示，各重组合获利情况如下表：\n",
    "|联盟 $s$|$\\{A\\}$|$\\{B\\}$|$\\{C\\}$|$\\{A,B\\}$|$\\{A,C\\}$|$\\{B,C\\}$|$\\{A,B,C\\}$|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|联盟收益 $v(s)$|0|0|1|6|5|3|10|\n",
    "\n",
    "根据以上信息，请站在甲的角度，估算甲应分得的利润，并填充以下的表格与 $\\psi_A$ （10分）：\n",
    "|包含A的联盟|$\\{A\\}$|$\\{A,B\\}$|$\\{A,C\\}$|$\\{A,B,C\\}$|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|联盟收益 $v(s)$|0|6|5|10|\n",
    "|A的边际贡献 $v(s)-v(s$ \\\\ $\\{A\\})$|||||\n",
    "|权重系数 $w(s)$|||||\n",
    "\n",
    "A应分得得利润 $\\psi_A=$\n",
    "\n",
    "并补充下面代码中的 `TODO` 部分，用以计算出A、B、C三人各自的应得利润（夏普利值）并填入下表（10分）。\n",
    "|商人|$A$|$B$|$C$|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|夏普利值 $\\psi$||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066923dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PLAYER = 3\n",
    "PLAYER_LIST = ['A', 'B', 'C']\n",
    "\n",
    "V_S = {\n",
    "    'A': 0,\n",
    "    'B': 0,\n",
    "    'C': 1,\n",
    "    'AB': 6,\n",
    "    'AC': 5,\n",
    "    'BC': 3,\n",
    "    'ABC': 10\n",
    "}\n",
    "\n",
    "# 计算边际贡献\n",
    "\n",
    "\n",
    "def cal_margin(coalition, player):\n",
    "    ################# TODO #################\n",
    "    margin = _\n",
    "    ############### END TODO ###############\n",
    "    return margin\n",
    "\n",
    "\n",
    "# 计算权重系数\n",
    "def cal_weights(coalition, n_player):\n",
    "    ################# TODO #################\n",
    "    weights = _\n",
    "    ############### END TODO ###############\n",
    "    return weights\n",
    "\n",
    "# 计算夏普利值\n",
    "\n",
    "\n",
    "def cal_shapley(n_player, player_list, v_s):\n",
    "    shapley_value_list = []\n",
    "    for i in range(n_player):\n",
    "        ################# TODO #################\n",
    "        shapley_value = _\n",
    "        ############### END TODO ###############\n",
    "        shapley_value_list.append(shapley_value)\n",
    "    return shapley_value_list\n",
    "\n",
    "\n",
    "# 测试入口\n",
    "shapley_values = cal_shapley(N_PLAYER, PLAYER_LIST, V_S)\n",
    "for i in range(N_PLAYER):\n",
    "    print(\"Shapley value for {} is: {}\".format(\n",
    "        PLAYER_LIST[i], shapley_values[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873c65f",
   "metadata": {},
   "source": [
    "## 矩阵博弈（共40分）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe539815",
   "metadata": {},
   "source": [
    "#### 初始化矩阵博弈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d57e4",
   "metadata": {},
   "source": [
    "让我们继续练习一的故事。在合作一段时间后，市场形势发生了巨大的变化，使得后续订单中A与B之间的利益产生了冲突。因此，A与B将重新评估继续参与联盟的可能性并逐渐调整策略。在该过程中，A，B均只有两种可能的动作 $\\{1,2\\}$，其中 $1$ 代表在下一个订单参与联盟，$2$ 代表不参与。二人不同的联合动作分别对应着各自不同的回报，具体可以用下面的矩阵博弈表示：\n",
    "\n",
    "$$\n",
    "\\mathbf{R}^A = \\left[\\begin{matrix}\n",
    "r^{A}_{11} & r^{A}_{12} \\\\\n",
    "r^{A}_{21} & r^{A}_{22}\n",
    "\\end{matrix}\\right] = \\left[\\begin{matrix}\n",
    "0 & 3 \\\\\n",
    "1 & 2\n",
    "\\end{matrix}\\right] \n",
    "\\quad \n",
    "\\mathbf{R}^B = \\left[\\begin{matrix}\n",
    "r^{B}_{11} & r^{B}_{12} \\\\\n",
    "r^{B}_{21} & r^{B}_{22}\n",
    "\\end{matrix}\\right] = \\left[\\begin{matrix}\n",
    "3 & 2 \\\\\n",
    "0 & 1\n",
    "\\end{matrix}\\right]\n",
    "\\quad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402bc0b",
   "metadata": {},
   "source": [
    "我们用 $\\alpha\\in [0,1]$ 来代表A选择动作 $1$ 的概率，用 $\\beta\\in [0,1]$ 来代表B选择动作 $1$ 的概率。因此，对于给定的联合策略 $(\\alpha, \\beta)$，我们可以用 $V^A(\\alpha, \\beta)$ 和 $V^B(\\alpha, \\beta)$ 来分别表示商人A与B的期望回报：\n",
    "\n",
    "$$\n",
    "\\begin{aligned} V^{A}(\\alpha, \\beta) &=\\alpha \\beta r^A_{11}+\\alpha(1-\\beta) r^A_{12}+(1-\\alpha) \\beta r^A_{21}+(1-\\alpha)(1-\\beta) r^A_{22} \\\\ &=u^A \\alpha \\beta+\\alpha\\left(r^A_{12}-r^A_{22}\\right)+\\beta\\left(r^A_{21}-r^A_{22}\\right)+r^A_{22} \\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned} V^{B}(\\alpha, \\beta) &=\\alpha \\beta r^B_{11}+\\alpha(1-\\beta) r^B_{12}+(1-\\alpha) \\beta r^B_{21}+(1-\\alpha)(1-\\beta) r^B_{22} \\\\ &=u^B \\alpha \\beta+\\alpha\\left(r^B_{12}-r^B_{22}\\right)+\\beta\\left(r^B_{21}-r^B_{22}\\right)+r^B_{22}\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\begin{aligned} u^A &=r^A_{11}-r^A_{12}-r^A_{21}+r^A_{22} \\\\  u^B &=r^B_{11}-r^B_{12}-r^B_{21}+r^B_{22} \\end{aligned}\n",
    "$$\n",
    "\n",
    "设初始策略 $\\alpha = 1.0$ 与 $\\beta = 0.1$，补充以下代码的 `TODO` 部分以完成该博弈的初始化（10分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27737319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# 计算u值\n",
    "\n",
    "\n",
    "def U(payoff):\n",
    "    ################# TODO #################\n",
    "    u = _\n",
    "    ############### END TODO ###############\n",
    "    return u\n",
    "\n",
    "\n",
    "# 计算给定联合策略下的A或B的期望回报\n",
    "def V(alpha, beta, payoff):\n",
    "    u = U(payoff)\n",
    "    ################# TODO #################\n",
    "    v = _\n",
    "    ############### END TODO ###############\n",
    "    return v\n",
    "\n",
    "\n",
    "# 初始化payoff矩阵\n",
    "payoff_a = np.array([[0., 3.],\n",
    "                     [1., 2.]])\n",
    "payoff_b = np.array([[3., 2.],\n",
    "                     [0., 1.]])\n",
    "\n",
    "# 初始策略\n",
    "pi_alpha = 1.\n",
    "pi_beta = 0.1\n",
    "\n",
    "# 纳什均衡策略\n",
    "################# TODO #################\n",
    "pi_alpha_nash = _  # nash strategy for player 1\n",
    "pi_beta_nash = _  # nash strategy for player 2\n",
    "############### END TODO ###############\n",
    "\n",
    "u_alpha = U(payoff_a)\n",
    "u_beta = U(payoff_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67585aac",
   "metadata": {},
   "source": [
    "#### 梯度上升法之 Infinitesimal Gredient Ascent (IGA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3732108",
   "metadata": {},
   "source": [
    "在A与B的磨合过程中，他们先尝试用常见的梯度上升法 [Infinitesimal Gradient Ascent (IGA)](https://www.sciencedirect.com/science/article/pii/S0004370202001212) 来逐步调整各自的策略。在每一次更新策略时，他们都会考虑改变当前策略对期望回报的影响。从数学上理解，该影响可以用期望回报对指定策略的偏微分（梯度）表示，具体如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial V^{A}(\\alpha, \\beta)}{\\partial \\alpha} &=\\beta u^A+\\left(r^A_{12}-r^A_{22}\\right) \\\\ \\frac{\\partial V^{B}(\\alpha, \\beta)}{\\partial \\beta} &=\\alpha u^B+\\left(r^B_{21}-r^B_{22}\\right). \\end{aligned}\n",
    "$$\n",
    "\n",
    "更新过程中，A与B均按照所求梯度的方向修改策略以增大期望回报，并考虑利用步长 $\\eta$ 来调整策略更新的幅度。设 $(\\alpha_k, \\beta_k)$ 是在第 $k$ 次更新时的联合策略，IGA的更新过程可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\alpha_{k+1}=\\alpha_{k}+\\eta \\frac{\\partial V^{A}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\alpha_{k}}} \\\\ {\\beta_{k+1}=\\beta_{k}+\\eta \\frac{\\partial V^{B}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\beta_{k}}}\\end{array}\n",
    "$$\n",
    "\n",
    "请根据该过程补充以下代码的 `TODO` 部分，实现IGA的更新过程（10分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2712912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IGA(pi_alpha,\n",
    "        pi_beta,\n",
    "        payoff_a,\n",
    "        payoff_b,\n",
    "        u_alpha,\n",
    "        u_beta,\n",
    "        iteration=1000,  # iteration number\n",
    "        eta=0.01  # step size\n",
    "        ):\n",
    "    pi_alpha_history = [pi_alpha]\n",
    "    pi_beta_history = [pi_beta]\n",
    "    pi_alpha_gradient_history = [0.]\n",
    "    pi_beta_gradient_history = [0.]\n",
    "    for i in range(iteration):\n",
    "        ################# TODO #################\n",
    "        pi_alpha_gradient = _\n",
    "        pi_beta_gradient = _\n",
    "        pi_alpha_next = _\n",
    "        pi_beta_next = _\n",
    "        ############### END TODO ###############\n",
    "        pi_alpha = max(0., min(1., pi_alpha_next))\n",
    "        pi_beta = max(0., min(1., pi_beta_next))\n",
    "        pi_alpha_gradient_history.append(pi_alpha_gradient)\n",
    "        pi_beta_gradient_history.append(pi_beta_gradient)\n",
    "        pi_alpha_history.append(pi_alpha)\n",
    "        pi_beta_history.append(pi_beta)\n",
    "    return pi_alpha_history, \\\n",
    "        pi_beta_history, \\\n",
    "        pi_alpha_gradient_history, \\\n",
    "        pi_beta_gradient_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192559d9",
   "metadata": {},
   "source": [
    "#### 梯度上升法之 WoLF-IGA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a0556",
   "metadata": {},
   "source": [
    "作为IGA的变种，[WoLF-IGA (Win or Learn Fast)](https://www.sciencedirect.com/science/article/pii/S0004370202001212) 则允许步长 $\\eta$ 在更新过程中自适应地改变。我们用 $\\alpha^{e}$ 和 $\\beta^{e}$ 分别表示A、B商人的纳什均衡策略，则WoLF-IGA的更新过程可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\alpha_{k+1}=\\alpha_{k}+\\eta_k^{A} \\frac{\\partial V^{A}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\alpha_{k}}} \\\\ {\\beta_{k+1}=\\beta_{k}+\\eta_k^{B}  \\frac{\\partial V^{B}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\beta_{k}}}\\end{array}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\eta_{k}^{A}=\\left\\{\\begin{array}{l}{\\eta_{\\min } \\text { if } V^A\\left(\\alpha_{k}, \\beta_{k}\\right)>V^A\\left(\\alpha^{e}, \\beta_{k}\\right)} \\\\ {\\eta_{\\max } \\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta_{k}^{B}=\\left\\{\\begin{array}{l}{\\eta_{\\min } \\text { if } V^B\\left(\\alpha_{k}, \\beta_{k}\\right)>V^B\\left(\\alpha_{k}, \\beta^{e}\\right)} \\\\ {\\eta_{\\max } \\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "同样地，完成下面代码中的 `TODO` 部分，以实现WoLF-IGA的更新过程（10分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074c4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WoLF_IGA(pi_alpha,\n",
    "             pi_beta,\n",
    "             payoff_a,\n",
    "             payoff_b,\n",
    "             u_alpha,\n",
    "             u_beta,\n",
    "             pi_alpha_nash,\n",
    "             pi_beta_nash,\n",
    "             iteration=1000,\n",
    "             eta_min=0.01,  # min step size\n",
    "             eta_max=0.04  # max step size\n",
    "             ):\n",
    "    pi_alpha_history = [pi_alpha]\n",
    "    pi_beta_history = [pi_beta]\n",
    "    pi_alpha_gradient_history = [0.]\n",
    "    pi_beta_gradient_history = [0.]\n",
    "    for i in range(iteration):\n",
    "        ################# TODO #################\n",
    "        pi_alpha_gradient = _\n",
    "        pi_beta_gradient = _\n",
    "        pi_alpha_next = _\n",
    "        pi_beta_next = _\n",
    "        ############### END TODO ###############\n",
    "        pi_alpha = max(0., min(1., pi_alpha_next))\n",
    "        pi_beta = max(0., min(1., pi_beta_next))\n",
    "        pi_alpha_gradient_history.append(pi_alpha_gradient)\n",
    "        pi_beta_gradient_history.append(pi_beta_gradient)\n",
    "        pi_alpha_history.append(pi_alpha)\n",
    "        pi_beta_history.append(pi_beta)\n",
    "    return pi_alpha_history, \\\n",
    "        pi_beta_history, \\\n",
    "        pi_alpha_gradient_history, \\\n",
    "        pi_beta_gradient_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3c953",
   "metadata": {},
   "source": [
    "#### 梯度上升法之 IGA-PP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74462f27",
   "metadata": {},
   "source": [
    "上面的两个方法在寻找商人A的新策略 $\\alpha_{k+1}$ 时，参考的是商人B在第 $k$ 轮时的策略 $\\beta_k$。而 [IGA-PP](https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1885) 则尝试对商人B在 $k+1$ 轮时的策略进行预测，并利用预测结果来更新商人A的策略，以寻求更加合理的联合策略 $(\\alpha_{k+1}, \\beta_{k+1})$。具体过程表示如下：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\alpha_{k+1}=\\alpha_{k}+\\eta\\frac{\\partial V^{A}\\left(\\alpha_{k}, \\beta_{k} + \\gamma \\partial_{\\beta}V^{B}\\left(\\alpha_{k}, \\beta_{k}\\right)  \\right)}{\\partial \\alpha_{k}}} \\\\ {\\beta_{k+1}=\\beta_{k}+\\eta  \\frac{\\partial V^{B}\\left(\\alpha_{k} + \\gamma \\partial_{\\alpha} V^{A}\\left(\\alpha_{k}, \\beta_{k} \\right) , \\beta_{k}\\right)}{\\partial \\beta_{k}}}\\end{array}\n",
    "$$\n",
    "\n",
    "完成下面代码中的 `TODO` 部分，以实现IGA-PP的更新过程（10分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3457860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IGA_PP(pi_alpha,\n",
    "           pi_beta,\n",
    "           payoff_a,\n",
    "           payoff_b,\n",
    "           u_alpha,\n",
    "           u_beta,\n",
    "           iteration=10000,\n",
    "           eta=0.01,  # step size\n",
    "           gamma=0.01  # step size for policy prediction\n",
    "           ):\n",
    "    pi_alpha_history = [pi_alpha]\n",
    "    pi_beta_history = [pi_beta]\n",
    "    pi_alpha_gradient_history = [0.]\n",
    "    pi_beta_gradient_history = [0.]\n",
    "    for i in range(iteration):\n",
    "        ################# TODO #################\n",
    "        pi_alpha_gradient = _\n",
    "        pi_beta_gradient = _\n",
    "        pi_alpha_next = _\n",
    "        pi_beta_next = _\n",
    "        ############### END TODO ###############\n",
    "        pi_alpha = max(0., min(1., pi_alpha_next))\n",
    "        pi_beta = max(0., min(1., pi_beta_next))\n",
    "        pi_alpha_gradient_history.append(pi_alpha_gradient)\n",
    "        pi_beta_gradient_history.append(pi_beta_gradient)\n",
    "        pi_alpha_history.append(pi_alpha)\n",
    "        pi_beta_history.append(pi_beta)\n",
    "    return pi_alpha_history, \\\n",
    "        pi_beta_history, \\\n",
    "        pi_alpha_gradient_history, \\\n",
    "        pi_beta_gradient_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa3d58",
   "metadata": {},
   "source": [
    "以下为验证与绘图代码，你可以通过运行以下代码以观察上述三种IGA算法在商人A与B的矩阵博弈中的表现。你可以尝试调整超参（如步长 $\\eta$）以了解更多的细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9f270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "FONTSIZE = 12\n",
    "\n",
    "# Tool to plot the learning dynamics\n",
    "\n",
    "\n",
    "def plot_dynamics(history_pi_0, history_pi_1, pi_alpha_gradient_history, pi_beta_gradient_history, title=''):\n",
    "    colors = range(len(history_pi_1))\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    scatter = ax.scatter(history_pi_0, history_pi_1, c=colors, s=1)\n",
    "    ax.scatter(0.5, 0.5, c='r', s=15., marker='*')\n",
    "    colorbar = fig.colorbar(scatter, ax=ax)\n",
    "    colorbar.set_label('Iterations', rotation=270, fontsize=FONTSIZE)\n",
    "\n",
    "    skip = slice(0, len(history_pi_0), 50)\n",
    "    ax.quiver(history_pi_0[skip],\n",
    "              history_pi_1[skip],\n",
    "              pi_alpha_gradient_history[skip],\n",
    "              pi_beta_gradient_history[skip],\n",
    "              units='xy', scale=10., zorder=3, color='blue',\n",
    "              width=0.007, headwidth=3., headlength=4.)\n",
    "\n",
    "    ax.set_ylabel(\"Policy of Player 2\", fontsize=FONTSIZE)\n",
    "    ax.set_xlabel(\"Policy of Player 1\", fontsize=FONTSIZE)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(title, fontsize=FONTSIZE+8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = ['IGA', 'WoLF-IGA', 'IGA-PP']\n",
    "\n",
    "for agent in agents:\n",
    "\n",
    "    if agent == 'IGA':\n",
    "        pi_alpha_history, \\\n",
    "            pi_beta_history, \\\n",
    "            pi_alpha_gradient_history, \\\n",
    "            pi_beta_gradient_history = IGA(pi_alpha,\n",
    "                                           pi_beta,\n",
    "                                           payoff_a,\n",
    "                                           payoff_b,\n",
    "                                           u_alpha,\n",
    "                                           u_beta,\n",
    "                                           iteration=1000,  # iteration number\n",
    "                                           eta=0.01  # step size\n",
    "                                           )\n",
    "    elif agent == 'WoLF-IGA':\n",
    "        pi_alpha_history, \\\n",
    "            pi_beta_history, \\\n",
    "            pi_alpha_gradient_history, \\\n",
    "            pi_beta_gradient_history = WoLF_IGA(pi_alpha,\n",
    "                                                pi_beta,\n",
    "                                                payoff_a,\n",
    "                                                payoff_b,\n",
    "                                                u_alpha,\n",
    "                                                u_beta,\n",
    "                                                pi_alpha_nash=pi_alpha_nash,\n",
    "                                                pi_beta_nash=pi_beta_nash,\n",
    "                                                iteration=1000,  # iteration number\n",
    "                                                eta_min=0.01,  # min step size\n",
    "                                                eta_max=0.04  # max step size\n",
    "                                                )\n",
    "\n",
    "    elif agent == 'IGA-PP':\n",
    "        pi_alpha_history, \\\n",
    "            pi_beta_history, \\\n",
    "            pi_alpha_gradient_history, \\\n",
    "            pi_beta_gradient_history = IGA_PP(pi_alpha,\n",
    "                                              pi_beta,\n",
    "                                              payoff_a,\n",
    "                                              payoff_b,\n",
    "                                              u_alpha,\n",
    "                                              u_beta,\n",
    "                                              iteration=10000,  # iteration number\n",
    "                                              eta=0.01,  # step size\n",
    "                                              gamma=0.01  # step size for policy prediction\n",
    "                                              )\n",
    "\n",
    "#   print(\"pi_alpha_history: \", pi_alpha_history)\n",
    "    plot_dynamics(pi_alpha_history,\n",
    "                  pi_beta_history,\n",
    "                  pi_alpha_gradient_history,\n",
    "                  pi_beta_gradient_history,\n",
    "                  agent)\n",
    "    print('{} Done'.format(agent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db08b4",
   "metadata": {},
   "source": [
    "## 随机博弈（共40分）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570b8b0",
   "metadata": {},
   "source": [
    "#### 初始化随机博弈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65898fe",
   "metadata": {},
   "source": [
    "某天，商人A与B收到了一批未曾接触过的新订单，每一个订单均分为多个不同的步骤，且每个步骤需要A、B二人通力合作才能完成，最终能取得的共同利润也与二人在这些步骤中所做的决策相关。由于A、B此前均无这类订单的经验，他们需要从零开始探索最佳的合作方案，以谋求利润的最大化。\n",
    "\n",
    "如果我们站在上帝视角，完成新订单的过程可以用下图中的随机博弈表示：\n",
    "![Stochastic Game](sg.png)\n",
    "该博弈一共包括5个阶段，其中中间矩阵代表博弈的初始状态，根据二人的联合动作存在着两个分支，分别导向最左最右两个终止阶段。如图所示，除初始与终止状态外，左右两个分支的中间阶段均为连续三次的的重复博弈。A与B均只有两种可能的动作 $\\{0,1\\}$ （A为行，B为列），且矩阵对应位置的数值代表相应阶段A、B执行联合动作后所共同获得的即时回报，其中的红色标志意味着A、B执行对应联合动作后博弈将提前结束。A、B所获的的总回报为各阶段即时回报之和。\n",
    "\n",
    "Note：从上帝视角看，不难看出该随机博弈的最优联合动作序列为 $\\{(0,0), (0,0), (0,0), (0,0), (1,1)\\}$，此时获得的最优总回报为8。但商人A、B并没有上帝视角，他们能观察到的信息仅包括当前所处的阶段与分支。\n",
    "\n",
    "在这个练习中，同学们需要利用所学的知识，实现属于自己的智能体，以模拟商人A和B探寻最佳合作方案（策略）的过程。 下面是初始化该随机博弈环境的代码，如有需要，环境的细节信息可以从代码中获得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b45cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class StochasticGame():\n",
    "    def __init__(self, episode_limit=5, good_branches=2, batch_size=None, **kwargs):\n",
    "        # Define the agents\n",
    "        self.n_agents = 2\n",
    "\n",
    "        self.episode_limit = episode_limit\n",
    "\n",
    "        # Define the internal state\n",
    "        self.steps = 0\n",
    "\n",
    "        r_matrix = [[1, 1], [1, 1]]\n",
    "        self.payoff_values = [r_matrix for _ in range(self.episode_limit)]\n",
    "        self.final_step_diff = [[1, 1], [1, 4]]\n",
    "\n",
    "        self.branches = 4\n",
    "        self.branch = 0\n",
    "\n",
    "        self.n_actions = len(self.payoff_values[0])\n",
    "\n",
    "        self.good_branches = good_branches\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Returns initial observations and states\"\"\"\n",
    "        self.steps = 0\n",
    "        self.branch = 0\n",
    "        return self.get_obs()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\" Returns reward, terminated, info \"\"\"\n",
    "        current_branch = 0\n",
    "        if (actions[0], actions[1]) == (0, 0):\n",
    "            current_branch = 0\n",
    "        if (actions[0], actions[1]) == (0, 1):\n",
    "            current_branch = 1\n",
    "        if (actions[0], actions[1]) == (1, 0):\n",
    "            current_branch = 2\n",
    "        if (actions[0], actions[1]) == (1, 1):\n",
    "            current_branch = 3\n",
    "\n",
    "        if self.steps == 0:\n",
    "            self.branch = current_branch\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        info[\"good_payoff\"] = 0\n",
    "        info[\"branch\"] = self.branch\n",
    "\n",
    "        if self.good_branches == 4:\n",
    "            reward = 1 if self.branch == current_branch else 0  # Need to follow your branch\n",
    "        elif self.good_branches == 2:\n",
    "            reward = 1 if self.branch in [\n",
    "                0, 3] and self.branch == current_branch else 0\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Environment not setup to handle {} good branches\".format(self.good_branches))\n",
    "\n",
    "        if self.episode_limit > 1 and self.steps == self.episode_limit - 1 and self.branch == 0:\n",
    "            info[\"good_payoff\"] = 1\n",
    "            reward = self.final_step_diff[actions[0]][actions[1]]\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        if self.steps < self.episode_limit and reward > 0:\n",
    "            terminated = False\n",
    "        else:\n",
    "            terminated = True\n",
    "\n",
    "        info[\"episode_limit\"] = False\n",
    "\n",
    "        # How often the joint-actions are taken\n",
    "        info[\"action_00\"] = 0\n",
    "        info[\"action_01\"] = 0\n",
    "        info[\"action_10\"] = 0\n",
    "        info[\"action_11\"] = 0\n",
    "        if (actions[0], actions[1]) == (0, 0):\n",
    "            info[\"action_00\"] = 1\n",
    "        if (actions[0], actions[1]) == (0, 1):\n",
    "            info[\"action_01\"] = 1\n",
    "        if (actions[0], actions[1]) == (1, 0):\n",
    "            info[\"action_10\"] = 1\n",
    "        if (actions[0], actions[1]) == (1, 1):\n",
    "            info[\"action_11\"] = 1\n",
    "\n",
    "        return self.get_obs(), [reward] * 2, [terminated] * 2, info\n",
    "\n",
    "    def get_obs(self):\n",
    "        \"\"\" Returns all agent observations in a list \"\"\"\n",
    "        one_hot_step = [0] * (self.episode_limit + 1 + self.branches)\n",
    "        one_hot_step[self.steps] = 1\n",
    "        one_hot_step[self.episode_limit + 1 + self.branch] = 1\n",
    "        return [tuple(one_hot_step) for _ in range(self.n_agents)]\n",
    "\n",
    "    def get_obs_agent(self, agent_id):\n",
    "        \"\"\" Returns observation for agent_id \"\"\"\n",
    "        return self.get_obs()[agent_id]\n",
    "\n",
    "    def get_obs_size(self):\n",
    "        \"\"\" Returns the shape of the observation \"\"\"\n",
    "        return len(self.get_obs_agent(0))\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.get_obs_agent(0)\n",
    "\n",
    "    def get_state_size(self):\n",
    "        \"\"\" Returns the shape of the state\"\"\"\n",
    "        return self.get_obs_size()\n",
    "\n",
    "    def get_total_actions(self):\n",
    "        \"\"\" Returns the total number of actions an agent could ever take \"\"\"\n",
    "        return self.n_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697d1c9",
   "metadata": {},
   "source": [
    "#### 随机策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b7c19",
   "metadata": {},
   "source": [
    "我们提供了一个简单的随机策略作为基类与例子方便大家了解环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "442bb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample(pi):\n",
    "    return np.random.choice(pi.size, size=1, p=pi)[0]\n",
    "\n",
    "\n",
    "def normalize(pi):\n",
    "    minprob = np.min(pi)\n",
    "    if minprob < 0.0:\n",
    "        pi -= minprob\n",
    "    pi /= np.sum(pi)\n",
    "\n",
    "\n",
    "class BaseQAgent:\n",
    "    def __init__(self, name, action_num=2, phi=0.01, gamma=0.95, episilon=0.1, **kwargs):\n",
    "        self.name = name\n",
    "        self.action_num = action_num\n",
    "        self.episilon = episilon\n",
    "        self.gamma = gamma\n",
    "        self.phi = phi\n",
    "        self.epoch = 0\n",
    "        self.Q = None\n",
    "        self.pi = defaultdict(\n",
    "            partial(np.random.dirichlet, [1.0] * self.action_num))\n",
    "\n",
    "    def done(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, exploration=False):\n",
    "        if exploration and random.random() < self.episilon:\n",
    "            return random.randint(0, self.action_num - 1)\n",
    "        else:\n",
    "            return sample(self.pi[observation])\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, observation, action, reward, next_observation, done):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_policy(self, observation, action):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd179a",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5bb25",
   "metadata": {},
   "source": [
    "作为马尔可夫决策场景中一个经典的单智能体算法，Q-Learning的策略更新过程可以表示为：\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow(1-\\phi) Q(s, a)+\\phi\\left(r+\\gamma V\\left(s^{\\prime}\\right)\\right)\n",
    "$$\n",
    "\n",
    "其中，\n",
    "$$\n",
    "V(s)=\\max\\left(\\left[Q(s, a)_{a \\in \\mathcal{A}}\\right]\\right)\n",
    "$$\n",
    "\n",
    "请同学先尝试着用基于Q-Learning的智能体来解决上面的随机博弈问题，补充下方代码中的 `TODO` 部分（10分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26147c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(BaseQAgent):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__('QAgent', **kwargs)\n",
    "        self.Q = defaultdict(partial(np.random.rand, self.action_num))\n",
    "        self.R = defaultdict(partial(np.zeros, self.action_num))\n",
    "        self.count_R = defaultdict(partial(np.zeros, self.action_num))\n",
    "\n",
    "    def done(self):\n",
    "        self.R.clear()\n",
    "        self.count_R.clear()\n",
    "\n",
    "    def update(self, observation, action, reward, next_observation, done):\n",
    "        self.count_R[observation][action] += 1.0\n",
    "        self.R[observation][action] += (reward - self.R[observation]\n",
    "                                        [action]) / self.count_R[observation][action]\n",
    "        Q = self.Q[observation]\n",
    "        V = self.val(next_observation)\n",
    "\n",
    "        # 请实现Q的更新过程\n",
    "        ################# TODO #################\n",
    "        self.Q[observation][action] = _\n",
    "        ############### END TODO ###############\n",
    "        self.update_policy(observation, action)\n",
    "        self.epoch += 1\n",
    "\n",
    "    def val(self, observation):\n",
    "        # 请实现V的计算\n",
    "        ################# TODO #################\n",
    "        v = _\n",
    "        ############### END TODO ###############\n",
    "        return v\n",
    "\n",
    "    def update_policy(self, observation, action):\n",
    "        Q = self.Q[observation]\n",
    "        self.pi[observation] = (Q == np.max(Q)).astype(np.double)\n",
    "        self.pi[observation] = self.pi[observation] / \\\n",
    "            np.sum(self.pi[observation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca78d5f7",
   "metadata": {},
   "source": [
    "以下为测试与绘图代码，你可以通过运行以下代码来观察你实现的智能体的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "139dd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def rollout(env, agents, exploration=True, max_episode=30000, log_episode_interval=500, verbose=False):\n",
    "    history_reward = []\n",
    "    state_n = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_count = 0\n",
    "    recorded_episodes = []\n",
    "    recorded_episode_reward = []\n",
    "    while episode_count < max_episode:\n",
    "        actions = np.array([agent.act(state, exploration)\n",
    "                           for state, agent in zip(state_n, agents)])\n",
    "        next_state_n, reward_n, done_n, _ = env.step(actions)\n",
    "        episode_reward += np.mean(reward_n)\n",
    "        for j, (state, reward, next_state, done, agent) in enumerate(zip(state_n, reward_n, next_state_n, done_n, agents)):\n",
    "            agent.update(state, actions[j], reward, next_state, done)\n",
    "        state_n = next_state_n\n",
    "        if np.all(done_n):\n",
    "            state_n = env.reset()\n",
    "            history_reward.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            episode_count += 1\n",
    "            if episode_count % log_episode_interval == 0:\n",
    "                recorded_episodes.append(episode_count)\n",
    "                episodes_mean_reward = np.mean(history_reward)\n",
    "                recorded_episode_reward.append(episodes_mean_reward)\n",
    "                history_reward = []\n",
    "                if verbose:\n",
    "                    print('Episodes {}, Reward {}'.format(\n",
    "                        episode_count, episodes_mean_reward))\n",
    "    return recorded_episodes, recorded_episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "721f37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_num = 2\n",
    "action_num = 2\n",
    "\n",
    "runs = 10\n",
    "log_episode_interval = 500\n",
    "train_recorded_episodes_log = []\n",
    "train_recorded_episode_reward_log = []\n",
    "test_recorded_episode_reward_log = []\n",
    "\n",
    "for i in range(runs):\n",
    "    ##################################### INITIALISATION ####################################\n",
    "    agents = []\n",
    "    env = StochasticGame()\n",
    "    for i in range(agent_num):\n",
    "        agent = QAgent(action_num=action_num)\n",
    "        agents.append(agent)\n",
    "\n",
    "    ####################################### TRAINING #######################################\n",
    "    train_recorded_episodes, train_recorded_episode_reward = rollout(env=env,\n",
    "                                                                     agents=agents,\n",
    "                                                                     exploration=True,\n",
    "                                                                     max_episode=30000,\n",
    "                                                                     log_episode_interval=log_episode_interval)\n",
    "    train_recorded_episodes_log.append(train_recorded_episodes)\n",
    "    train_recorded_episode_reward_log.append(train_recorded_episode_reward)\n",
    "\n",
    "    ####################################### TESTING #######################################\n",
    "    test_recorded_episodes, test_recorded_episode_reward = rollout(env=env,\n",
    "                                                                   agents=agents,\n",
    "                                                                   exploration=False,\n",
    "                                                                   max_episode=10,\n",
    "                                                                   log_episode_interval=1)\n",
    "    test_recorded_episode_reward_log.append(\n",
    "        np.mean(test_recorded_episode_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### TRAINING #######################################\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "df_reward = pd.DataFrame(train_recorded_episode_reward_log).melt()\n",
    "sns.lineplot(ax=ax, x='variable', y='value', data=df_reward)\n",
    "ax.set_title(f\"Train learning Curve for {runs} runs\")\n",
    "ax.set_ylabel(\"Episodic Reward\")\n",
    "ax.set_xlabel(\"Episodes * \" + str(log_episode_interval))\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "####################################### TESTING #######################################\n",
    "print(f'Test reward is (average over {runs} runs):', np.mean(\n",
    "    test_recorded_episode_reward_log))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed29a5",
   "metadata": {},
   "source": [
    "#### 多智能体学习算法扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12648b",
   "metadata": {},
   "source": [
    "观察Q-Learning的表现，是否有更好的方法去解决这个随机博弈问题呢？实现并训练一个完全属于你的智能体来更好地解决这个问题吧！简单与Q-Learning比较，分析下两种方法的异同与优缺。\n",
    "\n",
    "Note: 你可能需要考虑一些策略探索的方法。这里列出了一些可供参考的方法，当然，你的选择并不局限于此：\n",
    "1. [Minimax Q-Learning](https://arxiv.org/abs/1906.06659)\n",
    "2. [Nash Q-Learning](https://jmlr.csail.mit.edu/papers/volume4/hu03a/hu03a.pdf)\n",
    "3. [Friend-or-Foe Q-Learning](https://www.researchgate.net/publication/2933305_Friend-or-Foe_Q-learning_in_General-Sum_Games)\n",
    "4. [WOLF-PHC](https://www.sciencedirect.com/science/article/pii/S0004370202001212)\n",
    "5. [PGA-APP](https://www.researchgate.net/publication/220269223_Multi-Agent_Learning_with_Policy_Prediction)\n",
    "\n",
    "\n",
    "请补充下面代码的 `TODO` 部分（20分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你可以在这里按你的想法实现属于你的智能体！\n",
    "# 不过请保持接口能跟上面的环境对上，以便验证\n",
    "################# TODO #################\n",
    "class MyAgent(BaseQAgent):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__('CoolAgent', **kwargs)\n",
    "\n",
    "    def done(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, exploration):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, observation, action, reward, next_observation, done):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_policy(self, observation, action):\n",
    "        pass\n",
    "############### END TODO ###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60121d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请在这里训练和调试你的智能体，记得最后把你的智能体赋值给test_agents哦\n",
    "################# TODO #################\n",
    "agent_num = 2\n",
    "action_num = 2\n",
    "\n",
    "runs = 10\n",
    "log_episode_interval = 500\n",
    "train_recorded_episodes_log = []\n",
    "train_recorded_episode_reward_log = []\n",
    "\n",
    "for i in range(runs):\n",
    "    # 初始化环境与智能体\n",
    "    agents = []\n",
    "    env = StochasticGame()\n",
    "    for i in range(agent_num):\n",
    "        agent = MyAgent(action_num=action_num)\n",
    "        agents.append(agent)\n",
    "\n",
    "    # 训练智能体\n",
    "    train_recorded_episodes, train_recorded_episode_reward = rollout(env=env,\n",
    "                                                                     agents=agents,\n",
    "                                                                     exploration=True,\n",
    "                                                                     max_episode=30000,\n",
    "                                                                     log_episode_interval=log_episode_interval)\n",
    "    # 保存训练的log\n",
    "    train_recorded_episodes_log.append(train_recorded_episodes)\n",
    "    train_recorded_episode_reward_log.append(train_recorded_episode_reward)\n",
    "    ############### END TODO ###############\n",
    "\n",
    "    ####################################### TESTING #######################################\n",
    "    test_agents = agents\n",
    "    test_recorded_episode_reward_log = []\n",
    "    # 测试代码，请勿修改\n",
    "    test_env = StochasticGame()\n",
    "    test_recorded_episodes, test_recorded_episode_reward = rollout(env=test_env,\n",
    "                                                                   agents=test_agents,\n",
    "                                                                   exploration=False,\n",
    "                                                                   max_episode=10,\n",
    "                                                                   log_episode_interval=1)\n",
    "    test_recorded_episode_reward_log.append(\n",
    "        np.mean(test_recorded_episode_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### TRAINING #######################################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "df_reward = pd.DataFrame(train_recorded_episode_reward_log).melt()\n",
    "sns.lineplot(ax=ax, x='variable', y='value', data=df_reward)\n",
    "ax.set_title(f\"Train learning Curve for {runs} runs\")\n",
    "ax.set_ylabel(\"Episodic Reward\")\n",
    "ax.set_xlabel(\"Episodes * \" + str(log_episode_interval))\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Cool agent\\'s test reward is (average over {runs} runs):', np.mean(\n",
    "    test_recorded_episode_reward_log))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d22571",
   "metadata": {},
   "source": [
    "请在这里简短地描述下你的做法。重点阐述所选算法的核心思路，对比下其与Q-Learning的表现，分析其为何能在表现方面取得提升（小于500字）（10分）。\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
